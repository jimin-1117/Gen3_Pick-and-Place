# 🤖 LLM과 YOLOv5를 활용한 자연어 기반 로봇 Pick-and-Place 자동화 시스템


자연어 명령을 기반으로 객체를 인식하고, 로봇 매니퓰레이터를 제어하여 Pick-and-Place 작업을 수행하는 시스템입니다.  
본 프로젝트는 **LLM(OpenAI GPT)** 과 **YOLOv5**를 결합하여 직관적인 인간-로봇 상호작용을 가능하게 합니다.  

## 📌 소개
- 기존 매니퓰레이터는 사전에 정의된 반복 작업만 수행 가능 → 새로운 상황 대응 어려움.  
- 본 연구에서는 **LLM + YOLOv5** 결합을 통해 자연어 명령을 해석하고, 객체를 인식한 뒤 로봇이 자율적으로 Pick-and-Place 작업을 수행하도록 설계.  
- 사용자는 챗봇 인터페이스를 통해 **“사과 주스를 만들어줘”** 와 같은 명령을 내릴 수 있으며, 시스템은 이를 자동 실행합니다.


## ⚙️ 시스템 아키텍처
- **LLM (GPT-4)** : 자연어 명령 해석 및 필요한 객체 추출  
- **YOLOv5** : 객체 탐지 및 좌표 추출  
- **ROS2 + MoveIt** : 로봇 제어 및 궤적 생성  
- **Streamlit** : 사용자-로봇 상호작용 인터페이스

## 시스템 구조

<img width="647" height="235" alt="image" src="https://github.com/user-attachments/assets/ae307746-9bf2-43c6-8776-29d263c5da38" />





## ✨ 주요 기능
- **자연어 명령 해석** : GPT를 이용해 명령 속 객체와 작업 단계 추출  
- **객체 인식** : YOLOv5로 실시간 객체 탐지 및 좌표 변환  
- **로봇 경로 생성** : MoveIt 기반 직선/포물선 경로 생성  
- **Pick-and-Place 실행** : 로봇 매니퓰레이터와 그리퍼 제어  
- **사용자 인터페이스** : Streamlit 기반 챗봇 연동


## 🧪 실험 결과
- 테스트 환경:  
  - Manipulator: Kinova Gen3 (6DOF)  
  - Gripper: Robotiq 2F-140  
  - RGB-D 카메라: Kinova Gen3 End-Effector 부착  

- 명령: **“사과 주스를 만들어줘”**  
- 20회 테스트 결과: **95% 성공률**  
- 주요 오류 원인: 유사 객체(귤)를 사과로 잘못 인식 → 추가 학습으로 개선 가능


## 🚀 실험 영상










